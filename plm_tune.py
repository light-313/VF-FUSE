import os
import warnings
import torch
import json
from ray import tune
from ray.tune import CLIReporter
from ray.tune.schedulers import ASHAScheduler
from ray.tune.search.optuna import OptunaSearch
from ray.air import session
from plm_train import *

warnings.filterwarnings("ignore")
warnings.simplefilter("ignore", UserWarning)
warnings.simplefilter("ignore", FutureWarning)
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"


# ========= âœ… Ray Tune è®­ç»ƒå‡½æ•° (ä½¿ç”¨äº¤å‰éªŒè¯) =========
def train_tune(config, train_path, device, num_classes, prot5_path=None):
    # ä½¿ç”¨äº¤å‰éªŒè¯è®­ç»ƒï¼Œä¸å†éœ€è¦test_pathå‚æ•°
    # æ£€æŸ¥æ˜¯å¦ä¸ºèåˆæ¨¡å‹
    is_fusion_model = config["classifier_type"].lower() == "deltaattfusion"
    
    report = train_one_experiment(
        train_path=train_path,
        classifier_type=config["classifier_type"],
        log_dir=session.get_trial_dir(),
        hidden_dim=config["hidden_dim"],
        num_layers=config["num_layers"],
        dropout=config["dropout"],
        batch_size=config["batch_size"],
        num_epochs=config["num_epochs"],
        patience=config["patience"],
        learning_rate=config["learning_rate"],
        weight_decay=config["weight_decay"],
        optimizer=config["optimizer"],
        scheduler=config["scheduler"],
        device=device,
        
        # ç‰¹æœ‰å‚æ•°ï¼ˆå¦‚æœé…ç½®ä¸­å­˜åœ¨åˆ™ä¼ é€’ï¼‰
        rank=config.get('rank'),
        steps=config.get('steps'),
        
        # é¢å¤–å‚æ•°ï¼šProtT5ç‰¹å¾è·¯å¾„ï¼ˆä»…ç”¨äºèåˆæ¨¡å‹ï¼‰
        prot5_path=prot5_path,
        
        # äº¤å‰éªŒè¯å‚æ•°
        n_folds=5,  # ä½¿ç”¨5æŠ˜äº¤å‰éªŒè¯ï¼ŒåŠ å¿«è°ƒå‚é€Ÿåº¦
    )
    
    # æŠ¥å‘Šå¹³å‡F1åˆ†æ•°
    tune.report({
        "f1": report["f1"],
        "acc": report["acc"],
        "sn": report["sn"],
        "sp": report["sp"],
        "mcc": report["mcc"],
        "std_f1": report["f1_std"]  # æ·»åŠ F1æ ‡å‡†å·®
    })


def custom_trial_name(trial):
    return f"{trial.trainable_name}_{trial.trial_id}"


# ========= âœ… ä¸»å…¥å£ =========
if __name__ == "__main__":
    seed_everything(666)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    base_embed_dir = "/root/autodl-tmp/.autodl/embedding_data"
    save_base_dir = "/root/autodl-tmp/.autodl/prot5_tune_result"
    
    # ESMç‰¹å¾è·¯å¾„
    esm_train_path = os.path.join(base_embed_dir, "train_ba_esm2_t33_650M_UR50D_mean.h5")
    # ProtT5ç‰¹å¾è·¯å¾„
    prot5_train_path = "/root/autodl-tmp/train_ba_prot_features_modified.h5"
    

    # ä½¿ç”¨æ›´æ¿€è¿›çš„æ—©åœè°ƒåº¦å™¨ï¼ŒåŠ å¿«æœç´¢é€Ÿåº¦
    scheduler = ASHAScheduler(
        metric="f1",
        mode="max",
        max_t=30,          # æœ€å¤§epochs
        grace_period=5,    # è‡³å°‘è®­ç»ƒ5ä¸ªepochsæ‰è€ƒè™‘åœæ­¢
        reduction_factor=2 # æ¯æ¬¡å‡å°‘ä¸€åŠçš„trials
    )
    
    # å¢åŠ æ›´å¤šçš„æŒ‡æ ‡åˆ—ç”¨äºç›‘æ§
    reporter = CLIReporter(
        metric_columns=["f1", "acc", "sn", "sp", "mcc", "std_f1", "training_iteration"]
    )

    model_list = [
        # 'cnn','gru','transformer',
        # 'mlp','mamba',
        # 'bilstm','lstm', 'biomamaba', 'delta',
        # 'delta',
        'deltaattfusion'  # æ·»åŠ ç‰¹å¾èåˆæ¨¡å‹
    ]

    for model_name in model_list:
        print(f"ğŸ” å¼€å§‹è°ƒå‚: {model_name}")
        
        # ç¡®å®šä½¿ç”¨çš„ç‰¹å¾è·¯å¾„
        is_fusion_model = model_name.lower() == "deltaattfusion"
        train_path = esm_train_path  # ESMä½œä¸ºåŸºç¡€ç‰¹å¾

        # æ ¹æ®æ¨¡å‹ç±»å‹åˆ›å»ºåŸºç¡€é…ç½®
        base_config = {
            "classifier_type": model_name,  # å›ºå®šä¸ºå½“å‰æ¨¡å‹å
            "hidden_dim": tune.choice([128, 256, 512, 1024]),
            "num_layers": tune.choice([1, 2, 3, 4, 5,6,7]),
            "dropout": tune.uniform(0.1, 0.7),
            "batch_size": tune.choice([64, 128, 256, 512,1024]),
            "learning_rate": tune.loguniform(1e-5, 1e-3),
            "weight_decay": tune.loguniform(1e-6, 1e-3),
            "optimizer": tune.choice(["Adam", "AdamW",'SGD','RMSprop']),  # ç®€åŒ–ä¼˜åŒ–å™¨é€‰æ‹©
            "scheduler": tune.choice(["none", "step", "cosine"]),
            # å›ºå®šå‚æ•°
            "num_epochs": 30,  # å‡å°‘epochæ•°ä»¥åŠ å¿«è°ƒå‚
            "patience": 5,
        }
        
        # æ ¹æ®æ¨¡å‹ç±»å‹æ·»åŠ ç‰¹æœ‰å‚æ•°
        if model_name.lower() in ['delta', 'deltaattfusion']:
            base_config.update({
                'rank': tune.choice([4, 8, 16, 32]),
                'steps': tune.choice([1, 2, 3, 4]),
            })

        # åˆ›å»ºæœç´¢ç®—æ³•
        search_algo = OptunaSearch(metric="f1", mode="max")
        
        # è®¾ç½®è¯•éªŒæ•°é‡
        num_samples = 30 if model_name in ['delta', 'mamba', 'bimamba', 'deltaattfusion'] else 30
        
        print(f"å°†è¿›è¡Œ {num_samples} æ¬¡å‚æ•°æœç´¢...")
        
        # è¿è¡Œè°ƒå‚
        result = tune.run(
            tune.with_parameters(
                train_tune,
                train_path=train_path,
                device=device,
                num_classes=2,
                prot5_path=prot5_train_path,  # åªä¸ºèåˆæ¨¡å‹ä¼ é€’ProtT5è·¯å¾„
            ),
            search_alg=search_algo,
            scheduler=scheduler,
            config=base_config,
            num_samples=num_samples,
            resources_per_trial={"cpu": 2, "gpu": 0.15},  # å¢åŠ GPUèµ„æºä»¥åŠ å¿«è®­ç»ƒ
            progress_reporter=reporter,
            storage_path=os.path.join(save_base_dir, "trails", model_name),
            trial_dirname_creator=custom_trial_name,
            name=f"{model_name}_tune",
            keep_checkpoints_num=1,          # åªä¿ç•™æœ€å¥½çš„æ£€æŸ¥ç‚¹
            checkpoint_score_attr="f1",      # æ ¹æ®f1åˆ†æ•°ä¿ç•™æ£€æŸ¥ç‚¹
            verbose=1,                       # è¾“å‡ºæ›´è¯¦ç»†çš„æ—¥å¿—
        )

        # è·å–æœ€ä½³è¯•éªŒåŠå…¶é…ç½®
        best_trial = result.get_best_trial("f1", "max", "last")
        best_config = best_trial.config
        
        # ç»Ÿè®¡æœç´¢ç»“æœï¼Œè®¡ç®—å‚æ•°é¢‘ç‡
        param_stats = {}
        for param in ["hidden_dim", "num_layers", "optimizer", "scheduler"]:
            values = [t.config[param] for t in result.trials if t.last_result]
            from collections import Counter
            counts = Counter(values)
            param_stats[param] = counts
            
        print("\nå‚æ•°åˆ†å¸ƒç»Ÿè®¡:")
        for param, counts in param_stats.items():
            print(f"{param}: {dict(counts)}")

        print(f"\nâœ… {model_name} æœ€ä¼˜å‚æ•°: {best_config}")
        print(f"ğŸ“ˆ æœ€ç»ˆ F1: {best_trial.last_result['f1']:.4f} (std: {best_trial.last_result.get('std_f1', 0):.4f})")

        # ä¿å­˜æœ€ä¼˜å‚æ•°
        result_dir = os.path.join(save_base_dir, "result", model_name)
        os.makedirs(result_dir, exist_ok=True)
        with open(os.path.join(result_dir, "best_config.json"), "w") as f:
            json.dump(best_config, f, indent=4)

        # ========= âœ… å®Œæ•´è®­ç»ƒå¹¶ä¿å­˜ (ä½¿ç”¨æ›´å¤šæŠ˜å’Œæ›´å¤šepoch) =========
        print(f"ğŸš€ å¼€å§‹å®Œæ•´è®­ç»ƒï¼š{model_name}")
        final_report = train_one_experiment(
            train_path=train_path,
            classifier_type=best_config["classifier_type"],
            log_dir=result_dir,
            hidden_dim=best_config["hidden_dim"],
            num_layers=best_config["num_layers"],
            dropout=best_config["dropout"],
            batch_size=best_config["batch_size"],
            num_epochs=50,  # åœ¨æœ€ç»ˆè®­ç»ƒä¸­ä½¿ç”¨æ›´å¤šçš„epoch
            patience=6,    # å¢åŠ è€å¿ƒå‚æ•°
            learning_rate=best_config["learning_rate"],
            weight_decay=best_config["weight_decay"],
            optimizer=best_config["optimizer"],
            scheduler=best_config["scheduler"],
            device=device,
            # ç‰¹å®šæ¨¡å‹çš„å‚æ•°
            rank=best_config.get('rank'),
            steps=best_config.get('steps'),
            # é¢å¤–å‚æ•°ï¼šProtT5ç‰¹å¾è·¯å¾„ï¼ˆä»…ç”¨äºèåˆæ¨¡å‹ï¼‰
            prot5_path=prot5_train_path,
            # å®Œæ•´äº¤å‰éªŒè¯ä½¿ç”¨10æŠ˜
            n_folds=10,
        )

        # æ‰“å°å®Œæ•´è®­ç»ƒç»“æœ
        print(f"\nğŸ“Š {model_name} å®Œæ•´è®­ç»ƒç»“æœï¼š")
        print(f"ğŸ“ˆ å¹³å‡F1: {final_report['f1']:.4f} Â± {final_report['f1_std']:.4f}")
        print(f"ğŸ“ˆ å¹³å‡ACC: {final_report['acc']:.4f} Â± {final_report['acc_std']:.4f}")
        print(f"ğŸ“ˆ å¹³å‡SN: {final_report['sn']:.4f} Â± {final_report['sn_std']:.4f}")
        print(f"ğŸ“ˆ å¹³å‡SP: {final_report['sp']:.4f} Â± {final_report['sp_std']:.4f}")
        print(f"ğŸ“ˆ å¹³å‡MCC: {final_report['mcc']:.4f} Â± {final_report['mcc_std']:.4f}")
        
        # å¦‚æœæ˜¯èåˆæ¨¡å‹ï¼Œæ‰“å°æ³¨æ„åŠ›æƒé‡ä¿¡æ¯
        if is_fusion_model and "attention_weights" in final_report:
            print(f"ğŸ“Š å¹³å‡æ³¨æ„åŠ›æƒé‡ï¼šESM={final_report['attention_weights'][0]:.4f}, ProtT5={final_report['attention_weights'][1]:.4f}")
        
        # ä¿å­˜å®Œæ•´è®­ç»ƒç»“æœ
        with open(os.path.join(result_dir, "final_report.json"), "w") as f:
            json.dump(final_report, f, indent=4)
            
        # åˆ›å»ºæ‘˜è¦æ–‡ä»¶
        with open(os.path.join(result_dir, "summary.md"), "w") as f:
            f.write(f"# {model_name} æ¨¡å‹æ€§èƒ½æ‘˜è¦\n\n")
            f.write(f"## æœ€ä½³å‚æ•°\n")
            for param, value in best_config.items():
                f.write(f"- **{param}**: {value}\n")
            
            f.write(f"\n## äº¤å‰éªŒè¯æ€§èƒ½ (10æŠ˜)\n")
            f.write(f"- **F1**: {final_report['f1']:.4f} Â± {final_report['f1_std']:.4f}\n")
            f.write(f"- **ACC**: {final_report['acc']:.4f} Â± {final_report['acc_std']:.4f}\n")
            f.write(f"- **SN**: {final_report['sn']:.4f} Â± {final_report['sn_std']:.4f}\n")
            f.write(f"- **SP**: {final_report['sp']:.4f} Â± {final_report['sp_std']:.4f}\n")
            f.write(f"- **MCC**: {final_report['mcc']:.4f} Â± {final_report['mcc_std']:.4f}\n")
            
            # å¦‚æœæ˜¯èåˆæ¨¡å‹ï¼Œæ·»åŠ æ³¨æ„åŠ›æƒé‡ä¿¡æ¯
            if is_fusion_model and "attention_weights" in final_report:
                f.write(f"\n## ç‰¹å¾èåˆæƒé‡\n")
                f.write(f"- **ESMæƒé‡**: {final_report['attention_weights'][0]:.4f}\n")
                f.write(f"- **ProtT5æƒé‡**: {final_report['attention_weights'][1]:.4f}\n")
            
            f.write(f"\n## æœ€ä½³æŠ˜è¡¨ç°\n")
            f.write(f"- **æœ€ä½³æŠ˜**: {final_report['best_fold']}/10\n")
            f.write(f"- **æœ€ä½³F1**: {final_report['best_fold_f1']:.4f}\n")
            
        print(f"âœ… {model_name} å®Œæ•´è®­ç»ƒç»“æœå·²ä¿å­˜åˆ°: {result_dir}")
        print("="*80)